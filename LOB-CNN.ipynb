{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOB-CNN v1\n",
    "\n",
    "- 高频交易数据图像化建模与收益预测有效性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "import utils as _U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中证A50成分股\n",
    "# 信息来源：东方财富 20250324\n",
    "code_list = [\n",
    "    '688981sh', '603993sh', '603259sh', '601899sh', '601888sh', \n",
    "    '601816sh', '601766sh', '601668sh', '601600sh', '601318sh', \n",
    "    '601088sh', '601012sh', '600900sh', '600893sh', \n",
    "    '600887sh', '600660sh', '600585sh', '600519sh', '600436sh', \n",
    "    '600426sh', '600415sh', '600406sh', '600309sh', '600276sh', \n",
    "    '600176sh', '600036sh', '600031sh', '600030sh', '600028sh', \n",
    "    '600019sh', '600009sh', '300760sz', '300750sz', '300408sz', \n",
    "    '300124sz', '300122sz', '300015sz', '002714sz', '002594sz', \n",
    "    '002475sz', '002371sz', '002230sz', '002027sz', '000938sz', \n",
    "    '000792sz', '000725sz', '000333sz', '000063sz', '000002sz'\n",
    "    ]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 磁盘取数 + 初步处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "688981sh.csv saved. Shape: (112535, 110)\n",
      "603993sh.csv saved. Shape: (112915, 110)\n",
      "603259sh.csv saved. Shape: (112969, 110)\n",
      "601899sh.csv saved. Shape: (113636, 110)\n",
      "601888sh.csv saved. Shape: (113026, 110)\n",
      "601816sh.csv saved. Shape: (109638, 110)\n",
      "601766sh.csv saved. Shape: (111964, 110)\n",
      "601668sh.csv saved. Shape: (112059, 110)\n",
      "601600sh.csv saved. Shape: (113377, 110)\n",
      "601318sh.csv saved. Shape: (113140, 110)\n",
      "601088sh.csv saved. Shape: (111852, 110)\n",
      "601012sh.csv saved. Shape: (113857, 110)\n",
      "600941sh not found\n",
      "600900sh.csv saved. Shape: (102694, 110)\n",
      "600893sh.csv saved. Shape: (112947, 110)\n",
      "600887sh.csv saved. Shape: (113381, 110)\n",
      "600660sh.csv saved. Shape: (111974, 110)\n",
      "600585sh.csv saved. Shape: (112096, 110)\n",
      "600519sh.csv saved. Shape: (112195, 110)\n",
      "600436sh.csv saved. Shape: (111181, 110)\n",
      "600426sh.csv saved. Shape: (112122, 110)\n",
      "600415sh.csv saved. Shape: (110154, 110)\n",
      "600406sh.csv saved. Shape: (112640, 110)\n",
      "600309sh.csv saved. Shape: (112288, 110)\n",
      "600276sh.csv saved. Shape: (113358, 110)\n",
      "600176sh.csv saved. Shape: (112441, 110)\n",
      "600036sh.csv saved. Shape: (112684, 110)\n",
      "600031sh.csv saved. Shape: (113564, 110)\n",
      "600030sh.csv saved. Shape: (112830, 110)\n",
      "600028sh.csv saved. Shape: (112646, 110)\n",
      "600019sh.csv saved. Shape: (113182, 110)\n",
      "600009sh.csv saved. Shape: (112191, 110)\n",
      "300760sz.csv saved. Shape: (109545, 91)\n",
      "300750sz.csv saved. Shape: (110627, 91)\n",
      "300408sz.csv saved. Shape: (108400, 91)\n",
      "300124sz.csv saved. Shape: (110502, 91)\n",
      "300122sz.csv saved. Shape: (110421, 91)\n",
      "300015sz.csv saved. Shape: (110448, 91)\n",
      "002714sz.csv saved. Shape: (110615, 91)\n",
      "002594sz.csv saved. Shape: (110684, 91)\n",
      "002475sz.csv saved. Shape: (110580, 91)\n",
      "002371sz.csv saved. Shape: (110155, 91)\n",
      "002230sz.csv saved. Shape: (110367, 91)\n",
      "002027sz.csv saved. Shape: (110574, 91)\n",
      "000938sz.csv saved. Shape: (110166, 91)\n",
      "000792sz.csv saved. Shape: (110647, 91)\n",
      "000725sz.csv saved. Shape: (110697, 91)\n",
      "000333sz.csv saved. Shape: (110556, 91)\n",
      "000063sz.csv saved. Shape: (110535, 91)\n",
      "000002sz.csv saved. Shape: (110639, 91)\n",
      "688981sh.csv saved. Shape: (116914, 110)\n",
      "603993sh.csv saved. Shape: (118006, 110)\n",
      "603259sh.csv saved. Shape: (118033, 110)\n",
      "601899sh.csv saved. Shape: (118530, 110)\n",
      "601888sh.csv saved. Shape: (118098, 110)\n",
      "601816sh.csv saved. Shape: (115234, 110)\n",
      "601766sh.csv saved. Shape: (117113, 110)\n",
      "601668sh.csv saved. Shape: (117224, 110)\n",
      "601600sh.csv saved. Shape: (118961, 110)\n",
      "601318sh.csv saved. Shape: (118409, 110)\n",
      "601088sh.csv saved. Shape: (117291, 110)\n",
      "601012sh.csv saved. Shape: (118912, 110)\n",
      "600941sh not found\n",
      "600900sh.csv saved. Shape: (80064, 110)\n",
      "600893sh.csv saved. Shape: (117791, 110)\n",
      "600887sh.csv saved. Shape: (118158, 110)\n",
      "600660sh.csv saved. Shape: (117141, 110)\n",
      "600585sh.csv saved. Shape: (117170, 110)\n",
      "600519sh.csv saved. Shape: (117539, 110)\n",
      "600436sh.csv saved. Shape: (116877, 110)\n",
      "600426sh.csv saved. Shape: (116832, 110)\n",
      "600415sh.csv saved. Shape: (113178, 110)\n",
      "600406sh.csv saved. Shape: (117751, 110)\n",
      "600309sh.csv saved. Shape: (116912, 110)\n",
      "600276sh.csv saved. Shape: (118308, 110)\n",
      "600176sh.csv saved. Shape: (117314, 110)\n",
      "600036sh.csv saved. Shape: (118027, 110)\n",
      "600031sh.csv saved. Shape: (118686, 110)\n",
      "600030sh.csv saved. Shape: (117997, 110)\n",
      "600028sh.csv saved. Shape: (117540, 110)\n",
      "600019sh.csv saved. Shape: (117711, 110)\n",
      "600009sh.csv saved. Shape: (117159, 110)\n",
      "300760sz.csv saved. Shape: (114194, 91)\n",
      "300750sz.csv saved. Shape: (115652, 91)\n",
      "300408sz.csv saved. Shape: (113504, 91)\n",
      "300124sz.csv saved. Shape: (115427, 91)\n",
      "300122sz.csv saved. Shape: (115357, 91)\n",
      "300015sz.csv saved. Shape: (115454, 91)\n",
      "002714sz.csv saved. Shape: (115687, 91)\n",
      "002594sz.csv saved. Shape: (115675, 91)\n",
      "002475sz.csv saved. Shape: (115667, 91)\n",
      "002371sz.csv saved. Shape: (115181, 91)\n",
      "002230sz.csv saved. Shape: (115383, 91)\n",
      "002027sz.csv saved. Shape: (115628, 91)\n",
      "000938sz.csv saved. Shape: (115274, 91)\n",
      "000792sz.csv saved. Shape: (115640, 91)\n",
      "000725sz.csv saved. Shape: (115729, 91)\n",
      "000333sz.csv saved. Shape: (115536, 91)\n",
      "000063sz.csv saved. Shape: (115455, 91)\n",
      "000002sz.csv saved. Shape: (115574, 91)\n"
     ]
    }
   ],
   "source": [
    "# 获取 ~/ 目录下所有 snap_stkhf202101_*.sas7bdat 文件\n",
    "for year_month in ['202111', '202112']:\n",
    "    for code in code_list:\n",
    "        file_path = glob.glob(f'/mnt/sdb1/HF{year_month[:4]}/L2HF{year_month[2:]}_L2/snap_stkhf{year_month}_{code}.sas7bdat')\n",
    "        if file_path:\n",
    "            train_df = pd.read_sas(file_path[0], format='sas7bdat', encoding='utf-8')\n",
    "            train_df.to_csv(f'rawdata_L2HF{year_month[2:]}_L2/{code}.csv', index=False)\n",
    "            print(f'{code}.csv saved. Shape: {train_df.shape}')\n",
    "        else:\n",
    "            print(f'{code} not found')\n",
    "# 600941sh 中国移动上市日期 2022-01-05，晚于2021年11月"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "688981sh.csv saved. Shape: (104569, 69)\n",
      "603993sh.csv saved. Shape: (104395, 69)\n",
      "603259sh.csv saved. Shape: (104306, 69)\n",
      "601899sh.csv saved. Shape: (104291, 69)\n",
      "601888sh.csv saved. Shape: (104426, 69)\n",
      "601816sh.csv saved. Shape: (103177, 69)\n",
      "601766sh.csv saved. Shape: (104679, 69)\n",
      "601668sh.csv saved. Shape: (104617, 69)\n",
      "601600sh.csv saved. Shape: (104298, 69)\n",
      "601318sh.csv saved. Shape: (104302, 69)\n",
      "601088sh.csv saved. Shape: (104527, 69)\n",
      "601012sh.csv saved. Shape: (104276, 69)\n",
      "600900sh.csv saved. Shape: (95506, 69)\n",
      "600893sh.csv saved. Shape: (104335, 69)\n",
      "600887sh.csv saved. Shape: (104299, 69)\n",
      "600660sh.csv saved. Shape: (104491, 69)\n",
      "600585sh.csv saved. Shape: (104576, 69)\n",
      "600519sh.csv saved. Shape: (104630, 69)\n",
      "600436sh.csv saved. Shape: (104360, 69)\n",
      "600426sh.csv saved. Shape: (104492, 69)\n",
      "600415sh.csv saved. Shape: (103701, 69)\n",
      "600406sh.csv saved. Shape: (104440, 69)\n",
      "600309sh.csv saved. Shape: (104428, 69)\n",
      "600276sh.csv saved. Shape: (104296, 69)\n",
      "600176sh.csv saved. Shape: (104492, 69)\n",
      "600036sh.csv saved. Shape: (104341, 69)\n",
      "600031sh.csv saved. Shape: (104275, 69)\n",
      "600030sh.csv saved. Shape: (104361, 69)\n",
      "600028sh.csv saved. Shape: (104607, 69)\n",
      "600019sh.csv saved. Shape: (104444, 69)\n",
      "600009sh.csv saved. Shape: (104412, 69)\n",
      "300760sz.csv saved. Shape: (103596, 69)\n",
      "300750sz.csv saved. Shape: (104229, 69)\n",
      "300408sz.csv saved. Shape: (102645, 69)\n",
      "300124sz.csv saved. Shape: (104232, 69)\n",
      "300122sz.csv saved. Shape: (104175, 69)\n",
      "300015sz.csv saved. Shape: (104235, 69)\n",
      "002714sz.csv saved. Shape: (104231, 69)\n",
      "002594sz.csv saved. Shape: (104236, 69)\n",
      "002475sz.csv saved. Shape: (104234, 69)\n",
      "002371sz.csv saved. Shape: (104006, 69)\n",
      "002230sz.csv saved. Shape: (104134, 69)\n",
      "002027sz.csv saved. Shape: (104233, 69)\n",
      "000938sz.csv saved. Shape: (104078, 69)\n",
      "000792sz.csv saved. Shape: (104232, 69)\n",
      "000725sz.csv saved. Shape: (104236, 69)\n",
      "000333sz.csv saved. Shape: (104234, 69)\n",
      "000063sz.csv saved. Shape: (104228, 69)\n",
      "000002sz.csv saved. Shape: (104235, 69)\n",
      "688981sh.csv saved. Shape: (109304, 69)\n",
      "603993sh.csv saved. Shape: (109200, 69)\n",
      "603259sh.csv saved. Shape: (109054, 69)\n",
      "601899sh.csv saved. Shape: (109060, 69)\n",
      "601888sh.csv saved. Shape: (109233, 69)\n",
      "601816sh.csv saved. Shape: (108533, 69)\n",
      "601766sh.csv saved. Shape: (109501, 69)\n",
      "601668sh.csv saved. Shape: (109272, 69)\n",
      "601600sh.csv saved. Shape: (109043, 69)\n",
      "601318sh.csv saved. Shape: (109063, 69)\n",
      "601088sh.csv saved. Shape: (109231, 69)\n",
      "601012sh.csv saved. Shape: (109020, 69)\n",
      "600900sh.csv saved. Shape: (73111, 69)\n",
      "600893sh.csv saved. Shape: (109156, 69)\n",
      "600887sh.csv saved. Shape: (109041, 69)\n",
      "600660sh.csv saved. Shape: (109296, 69)\n",
      "600585sh.csv saved. Shape: (109236, 69)\n",
      "600519sh.csv saved. Shape: (109316, 69)\n",
      "600436sh.csv saved. Shape: (109310, 69)\n",
      "600426sh.csv saved. Shape: (109058, 69)\n",
      "600415sh.csv saved. Shape: (107077, 69)\n",
      "600406sh.csv saved. Shape: (109096, 69)\n",
      "600309sh.csv saved. Shape: (109192, 69)\n",
      "600276sh.csv saved. Shape: (109054, 69)\n",
      "600176sh.csv saved. Shape: (109227, 69)\n",
      "600036sh.csv saved. Shape: (109037, 69)\n",
      "600031sh.csv saved. Shape: (109027, 69)\n",
      "600030sh.csv saved. Shape: (109114, 69)\n",
      "600028sh.csv saved. Shape: (109378, 69)\n",
      "600019sh.csv saved. Shape: (109238, 69)\n",
      "600009sh.csv saved. Shape: (109256, 69)\n",
      "300760sz.csv saved. Shape: (108064, 69)\n",
      "300750sz.csv saved. Shape: (108967, 69)\n",
      "300408sz.csv saved. Shape: (107464, 69)\n",
      "300124sz.csv saved. Shape: (108929, 69)\n",
      "300122sz.csv saved. Shape: (108915, 69)\n",
      "300015sz.csv saved. Shape: (108933, 69)\n",
      "002714sz.csv saved. Shape: (108974, 69)\n",
      "002594sz.csv saved. Shape: (108972, 69)\n",
      "002475sz.csv saved. Shape: (108974, 69)\n",
      "002371sz.csv saved. Shape: (108855, 69)\n",
      "002230sz.csv saved. Shape: (108852, 69)\n",
      "002027sz.csv saved. Shape: (108973, 69)\n",
      "000938sz.csv saved. Shape: (108803, 69)\n",
      "000792sz.csv saved. Shape: (108973, 69)\n",
      "000725sz.csv saved. Shape: (108974, 69)\n",
      "000333sz.csv saved. Shape: (108971, 69)\n",
      "000063sz.csv saved. Shape: (108966, 69)\n",
      "000002sz.csv saved. Shape: (108969, 69)\n"
     ]
    }
   ],
   "source": [
    "level2_col_list = []\n",
    "for l in range(1, 11):\n",
    "    BidPr_col = f'BidPr{l}'\n",
    "    BidVol_col = f'BidVol{l}'\n",
    "    AskPr_col = f'AskPr{l}'\n",
    "    AskVol_col = f'AskVol{l}'\n",
    "    level2_col_list.extend([BidPr_col, BidVol_col, AskPr_col, AskVol_col])\n",
    "    \n",
    "used_cols = [\n",
    "    'datetime', 'Exchflg', 'Code', 'Code_Mkt', 'Qdate', 'QTime', 'InstrumentStatus', 'Trdirec',\n",
    "    'PrevClPr', 'OpPr', 'HiPr', 'LoPr', 'Tprice', 'Tvolume', 'Tsum', 'Tdeals', 'TVolume_accu', 'TSum_accu', 'Tdeals_accu',\n",
    "    'TotBidVol', 'WghtAvgBidPr', 'TotAskVol', 'WghtAvgAskPr',\n",
    "    'Absspread', 'Respread', 'Abseffspread', 'Reeffspread', 'Depth1', 'Depth2'\n",
    "]\n",
    "used_cols.extend(level2_col_list)\n",
    "\n",
    "for year_month in ['202111', '202112']:\n",
    "    for code in code_list:\n",
    "        train_df = pd.read_csv(f'rawdata_L2HF{year_month[2:]}_L2/{code}.csv')\n",
    "\n",
    "        # convert all df in dfs to datetime\n",
    "        date = pd.to_datetime(train_df['Qdate'])\n",
    "        time = pd.to_timedelta(train_df['QTime'], unit='s')\n",
    "        train_df['datetime'] = date + time\n",
    "        \n",
    "        # select used cols\n",
    "        train_df = train_df[used_cols]\n",
    "        train_df = _U.trading_time_slice(train_df)\n",
    "        train_df.to_csv(f'data_{year_month}/{code}.csv', index=False)\n",
    "        print(f'{code}.csv saved. Shape: {train_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_cols = [f'BidPr{i}' for i in range(1, 6)] + [f'AskPr{i}' for i in range(1, 6)]\n",
    "price_cols += [f'BidPr{i}_lag{t}' for i in range(1, 11) for t in range(1, 10)]\n",
    "price_cols += [f'AskPr{i}_lag{t}' for i in range(1, 11) for t in range(1, 10)]\n",
    "# for code in code_list:\n",
    "#     df = pd.read_csv(f'data_202111/{code}.csv')\n",
    "    \n",
    "price_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataGen: Image + Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_new_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    计算新的特征\n",
    "    \"\"\"\n",
    "    df['mid_price'] = (df['BidPr1'] + df['AskPr1']) / 2\n",
    "    \n",
    "    pred_cnt = 5\n",
    "    df['TWAP_mid'] = df['mid_price'].rolling(window=pred_cnt).mean()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104478/104478 [02:15<00:00, 773.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600176sh.csv loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# imaging data\n",
    "code_list = [\n",
    "    '600176sh', \n",
    "]\n",
    "folder_path = 'data_202111/'\n",
    "record_cnt = 5\n",
    "pred_cnt = 5\n",
    "is_binary = True\n",
    "\n",
    "image_list = []\n",
    "label_list = []\n",
    "for code in code_list:\n",
    "    train_df = pd.read_csv(f'{folder_path}{code}.csv')\n",
    "    train_df['datetime'] = pd.to_datetime(train_df['datetime'])\n",
    "    # train_df = train_df[train_df['datetime'] <= '2021-12-10 23:59:59']  # 测试集初步只取前10天\n",
    "    train_df = calc_new_features(train_df)\n",
    "    train_df.dropna(axis=0, inplace=True)\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    for i in tqdm.tqdm(range(len(train_df) - (record_cnt + pred_cnt))):\n",
    "        single_entry = _U.single_image(train_df.iloc[i:i + record_cnt + pred_cnt], record_cnt, pred_cnt, is_binary)\n",
    "        image_list.append(single_entry[0])\n",
    "        label_list.append(single_entry[1])\n",
    "    print(f'{code}.csv loaded.')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- getting `df` directly without saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104478, 616)\n"
     ]
    }
   ],
   "source": [
    "image_df = pd.DataFrame({\n",
    "    'image': image_list,\n",
    "    'label': label_list\n",
    "})\n",
    "\n",
    "# Flatten images and create a new dataframe\n",
    "flatten_data = []\n",
    "for i in range(len(image_df)):\n",
    "    flattened_image = image_df.loc[i, 'image'].flatten()\n",
    "    label = image_df.loc[i, 'label']\n",
    "    flatten_data.append(np.concatenate(([label], flattened_image)))  # Label in first column\n",
    "    \n",
    "columns = ['label'] + [f'pixel_{i}' for i in range(41 * 15)]\n",
    "flatten_df = pd.DataFrame(flatten_data, columns=columns)\n",
    "print(flatten_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = flatten_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save and downsample  `image_list` and `label_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "(10000, 616)\n",
      "log saved to image_dataset_20250325_1447.json\n"
     ]
    }
   ],
   "source": [
    "image_df = pd.DataFrame({\n",
    "    'image': image_list,\n",
    "    'label': label_list\n",
    "})\n",
    "\n",
    "sample_size = 10000\n",
    "random_indices = np.random.choice(image_df.shape[0], sample_size, replace=False)\n",
    "print(len(random_indices))\n",
    "# Flatten images and create a new dataframe\n",
    "flatten_data = []\n",
    "for i in random_indices:\n",
    "    flattened_image = image_df.loc[i, 'image'].flatten()\n",
    "    label = image_df.loc[i, 'label']\n",
    "    flatten_data.append(np.concatenate(([label], flattened_image)))  # Label in first column\n",
    "\n",
    "columns = ['label'] + [f'pixel_{i}' for i in range(41 * 15)]\n",
    "flatten_df = pd.DataFrame(flatten_data, columns=columns)\n",
    "print(flatten_df.shape)\n",
    "\n",
    "# Save to CSV\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "flatten_df.to_csv(f'image_dataset_{current_time}.csv', index=False)\n",
    "\n",
    "# write a log in JSON for image_df\n",
    "import json\n",
    "log = {\n",
    "    'original_data_folder_path': folder_path,\n",
    "    'code_list': code_list,\n",
    "    'record_cnt': record_cnt,\n",
    "    'pred_cnt': pred_cnt,\n",
    "    'ori_shape': image_df.shape,\n",
    "    'sample_size': sample_size,\n",
    "    'datetime': current_time\n",
    "}\n",
    "log_path = f'image_dataset_{current_time}.json'\n",
    "with open(log_path, 'w') as f:\n",
    "    json.dump(log, f, indent=4)\n",
    "print(f'log saved to {log_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load downsampled `image_df` and re-structure the np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 616)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('image_dataset_20250324_2212.csv')\n",
    "print(train_df.shape)\n",
    "\n",
    "# Prepare data for CNN (reshape 615 -> 41x15)\n",
    "features = train_df.iloc[:, 1:].values\n",
    "train_X_cnn = features.reshape(-1, 41, 15)  # Shape: (num_samples, 41, 15)\n",
    "\n",
    "train_labels = train_df.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 616) (10000, 616)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('image_dataset_20250324_2212.csv')\n",
    "test_df = pd.read_csv('image_dataset_20250325_1447.csv')\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104478, 616) (37973, 616)\n"
     ]
    }
   ],
   "source": [
    "train_df.dropna(axis=0, inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.dropna(axis=0, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution: \n",
      "label\n",
      "0.0    0.843632\n",
      "1.0    0.156368\n",
      "Name: proportion, dtype: float64\n",
      "Test label distribution: \n",
      "label\n",
      "0.0    32656\n",
      "1.0     5317\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preprare data\n",
    "train_X_flat = train_df.drop(columns=['label']).values\n",
    "train_X_cnn = train_X_flat.reshape(-1, 41, 15)  # Shape: (num_samples, 41, 15)\n",
    "train_labels = train_df['label'].values \n",
    "train_labels = train_labels.astype(int)\n",
    "\n",
    "test_X_flat = test_df.drop(columns=['label']).values\n",
    "test_X_cnn = test_X_flat.reshape(-1, 41, 15)  # Shape: (num_samples, 41, 15)\n",
    "test_labels = test_df['label'].values\n",
    "test_labels = test_labels.astype(int)\n",
    "\n",
    "print(f\"Train label distribution: \\n{train_df['label'].value_counts(normalize=True)}\")\n",
    "print(f\"Test label distribution: \\n{test_df['label'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample 10000 data from train_df and test_df\n",
    "sample_size = 10000\n",
    "random_indices_train = np.random.choice(train_df.shape[0], sample_size, replace=False)\n",
    "random_indices_test = np.random.choice(test_df.shape[0], sample_size, replace=False)\n",
    "\n",
    "train_X_flat = train_X_flat[random_indices_train]\n",
    "test_X_flat = test_X_flat[random_indices_test]\n",
    "\n",
    "train_labels = train_labels[random_indices_train]\n",
    "test_labels = test_labels[random_indices_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression (SGD) ---\n",
      "Time elapsed:  1.7294812202453613 (s)\n",
      "{'Model': 'Logistic Regression (SGD)', 'Accuracy': 0.8377, 'Precision': 0.32532051282051283, 'Recall': 0.14448398576512456, 'F1': 0.20009857072449483} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qbzhou21/.conda/envs/LOB-CNN/lib/python3.9/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Linear SVM ---\n",
      "Time elapsed:  93.42627429962158 (s)\n",
      "{'Model': 'Linear SVM', 'Accuracy': 0.8547, 'Precision': 0.38235294117647056, 'Recall': 0.05551601423487545, 'F1': 0.09695463020509633} \n",
      "\n",
      "--- XGBoost ---\n",
      "Time elapsed:  269.83997535705566 (s)\n",
      "{'Model': 'XGBoost', 'Accuracy': 0.8575, 'Precision': 0.4758454106280193, 'Recall': 0.1402135231316726, 'F1': 0.2166025288620121} \n",
      "\n",
      "--- MLP ---\n",
      "Time elapsed:  2.337718963623047 (s)\n",
      "{'Model': 'MLP', 'Accuracy': 0.8573, 'Precision': 0.36904761904761907, 'Recall': 0.02206405693950178, 'F1': 0.041638683680322364} \n",
      "\n",
      "Report saved to: reports/binary_Traditional_unbalanced_20250325_1644.csv\n",
      "                       Model  Accuracy  Precision    Recall        F1\n",
      "2                    XGBoost    0.8575   0.475845  0.140214  0.216603\n",
      "0  Logistic Regression (SGD)    0.8377   0.325321  0.144484  0.200099\n",
      "1                 Linear SVM    0.8547   0.382353  0.055516  0.096955\n",
      "3                        MLP    0.8573   0.369048  0.022064  0.041639\n"
     ]
    }
   ],
   "source": [
    "# traditional ML pipeline\n",
    "dataset = {\n",
    "    'train_X': train_X_flat,\n",
    "    'train_y': train_labels,\n",
    "    'test_X': test_X_flat,\n",
    "    'test_y': test_labels\n",
    "}\n",
    "df_results = _U.traditional_ml_pipeline(dataset, balance=False, data_type='num')\n",
    "print(df_results.sort_values('F1', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare to original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imaging data\n",
    "code_list = [\n",
    "    '600176sh', \n",
    "]\n",
    "folder_path = 'data_202111/'\n",
    "record_cnt = 5\n",
    "pred_cnt = 5\n",
    "is_binary = True\n",
    "\n",
    "for code in code_list:\n",
    "    train_df = pd.read_csv(f'{folder_path}{code}.csv')\n",
    "    train_df['datetime'] = pd.to_datetime(train_df['datetime'])\n",
    "    # train_df = train_df[train_df['datetime'] <= '2021-12-10 23:59:59']  # 测试集初步只取前10天\n",
    "    train_df = calc_new_features(train_df)\n",
    "    train_df.dropna(axis=0, inplace=True)\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    print(f'{code}.csv loaded.')\n",
    "\n",
    "folder_path = 'data_202112/'\n",
    "for code in code_list:\n",
    "    test_df = pd.read_csv(f'{folder_path}{code}.csv')\n",
    "    test_df['datetime'] = pd.to_datetime(test_df['datetime'])\n",
    "    test_df = test_df[test_df['datetime'] <= '2021-12-10 23:59:59']  # 测试集初步只取前10天\n",
    "    test_df = calc_new_features(test_df)\n",
    "    test_df.dropna(axis=0, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "    print(f'{code}.csv loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lob_deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
